{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w3_Quiz_NLP_NLU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaykrishna2013/NLP/blob/main/w3_Quiz_NLP_NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdR2I2GktWFP"
      },
      "source": [
        "<font color=brown><i><b>Copyright Information:</b></i> All course material is copyrighted by the author and Stanford University with all rights reserved. The material may not be reproduced or distributed without an explicit written permission of the author. Stanford University also has a policy that one should know before audio/video recording. Do not post material online.</font>\n",
        "\n",
        "# **w3 Quiz. NLP and NLU**\n",
        "\n",
        "Make a copy of this Colab Notebook with the **starter code** below and continue building your solution **in Colab** (not another Python environment) to assure an exact environment and matching solutions.\n",
        "\n",
        "Prof. Melnikov's video in module 3 demonstrates several ways to **tokenize sentences** and **words**. Here we evaluate their effectiveness in reducing the dimensionality of the vocabulary, while maintaining the \"quality\" of the tokens. We define \"quality\" with a binary function with *high* quality for words found in some commonly accepted vocabulary (such as [**Brown corpus**](https://www.nltk.org/book/ch02.html) from **NLTK library**). Other words are considered as *low* quality. This will require Python **set operations**, since we need to check whether a given word is in the vocabulary set or not.\n",
        "\n",
        "Most evaluations here are done one the first 100 posts from each of the [**20 Newsgroups corpus**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) in **Scikit-Learn library**. We then use the average metric (**median**, in this case) to compute the final quantity, which you will submit to Canvas Quiz.\n",
        "\n",
        "This quiz is a sequence of small projects, requiring computation of values. Do not round any values. If stuck for 10-15 minutes, raise a question in the w3 Discussion Q&A. See Syllabus for **Posting Guidelines**. In particular, describe your situation and what you have tried **without posting your code**.\n",
        "\n",
        "To ease code readability  and avoid confusion, we use the following prefix convention for key variables: \n",
        "1. `s`=string, `n`=number, `b`=Boolean\n",
        "1. `Ls`=list of strings, `Ln`=list of numbers, `Ss`=set of strings, `As`=NumPy array of strings, `Ds`=dictionary of string values\n",
        "1. `LLs`=list of lists of strings, `LTs`=list of tuples of strings, etc.\n",
        "1. `df`=Pandas dataframe or series, \n",
        "\n",
        "<p><font color=gray><i>Hint</i>: Refer to videos in Module 3 and some Python refresher videos of Corey Schafer.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8k-vxn4q62h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e96901-c3d2-40e9-9445-f82d0929a3eb"
      },
      "source": [
        "!pip -q install contractions   # quietly install contractions package\n",
        "# allows multiple outputs from a single Colab code cell:\n",
        "from IPython.core.interactiveshell import InteractiveShell  \n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import sys, matplotlib.pylab as plt, re, platform, matplotlib\n",
        "import numpy as np, pandas as pd, nltk, sklearn, spacy, unicodedata, contractions \n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from pprint import pprint\n",
        "tmp = nltk.download(['brown', 'stopwords','punkt','wordnet'], quiet=True) # See https://www.nltk.org/book/ch02.html\n",
        "LsStopWords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Increase viewable area of Pandas tables, NumPy arrays, plots\n",
        "pd.set_option('max_rows', 5, 'max_columns', 500, 'max_colwidth', 1, 'precision', 2)\n",
        "np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True)\n",
        "plt.rcParams['figure.figsize'] = [16, 4]\n",
        "\n",
        "def LoadNews(cat=['sci.space'], TopN=100):\n",
        "    '''Function to load a string of news posts for the specified categories. Returns: TopN concatenated news'''\n",
        "    Rem = ('headers', 'footers', 'quotes')  # remove these fields from result set\n",
        "    bunch = fetch_20newsgroups(categories=cat, subset='test', shuffle=False, remove=Rem)\n",
        "    return '\\n'.join(bunch.data[:TopN])  # save first 100 posts concatenated as a single string.\n",
        "\n",
        "# See doc: https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset\n",
        "# We preload string variables containing concatenated news posts \n",
        "sNews = LoadNews(['sci.space'])   # a string. news from space ;)  \n",
        "LsTgtNames = list(fetch_20newsgroups().target_names)   # names of 20 newsgroups\n",
        "\n",
        "pso = nltk.stem.PorterStemmer()       # instantiates Porter Stemmer object\n",
        "wlo = nltk.stem.WordNetLemmatizer()   # instantiates WordNet lemmatizer object\n",
        "SsBrownVcb = set(brown.words())       # Vocabulary of 56057 words in Brown Corpus\n",
        "\n",
        "# store sentence tokenizers' results as a list of lists or strings:\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "LLsST =  [sNews.split('. ')] \\\n",
        "    + [nltk.sent_tokenize(sNews)] \\\n",
        "    + [nltk.tokenize.PunktSentenceTokenizer().tokenize(sNews)] \\\n",
        "    + [[n.text for n in nlp(sNews).sents]]\n",
        "\n",
        "# store word tokenizers' results as a list of lists of strings\n",
        "LLsWT = [sNews.split()] \\\n",
        "    + [nltk.RegexpTokenizer(pattern=r\"\\s+\", gaps=True ).tokenize(sNews)] \\\n",
        "    + [nltk.RegexpTokenizer(pattern=r\"\\s+\", gaps=True ).tokenize(sNews)] \\\n",
        "    + [nltk.WhitespaceTokenizer().tokenize(sNews)] \\\n",
        "    + [nltk.RegexpTokenizer(pattern=r\"\\w+\", gaps=False).tokenize(sNews)] \\\n",
        "    + [nltk.word_tokenize(sNews)] \\\n",
        "    + [nltk.TreebankWordTokenizer().tokenize(sNews)] \\\n",
        "    + [[t.text for t in nlp(sNews)]] \\\n",
        "    + [nltk.tokenize.toktok.ToktokTokenizer().tokenize(sNews)] \\\n",
        "    + [nltk.WordPunctTokenizer().tokenize(sNews)]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 266kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 327kB 11.8MB/s \n",
            "\u001b[?25h  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM7jr96uv5_-"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAFHkUTPgViv",
        "outputId": "dda2f714-49e3-463d-f98f-10534c27bba2"
      },
      "source": [
        "# Example of news article\r\n",
        "pprint(sNews[0:100])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\"I'm afraid I was not able to find the GIFs... is the list \\n\"\n",
            " 'updated weekly, perhaps, or am I just mis')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDFXaBILgjFP",
        "outputId": "bb500c9d-d683-4edc-8bb0-81f28de4d66d"
      },
      "source": [
        "# Examples of parsed sentences by different tokenizers\r\n",
        "[{i:LLsST[j][i][:100] for i in [0,1]} for j in range(len(LLsST))]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{0: \"I'm afraid I was not able to find the GIFs..\",\n",
              "  1: 'is the list \\nupdated weekly, perhaps, or am I just missing something?\\n\\nThe forces and accelerations '},\n",
              " {0: \"I'm afraid I was not able to find the GIFs... is the list \\nupdated weekly, perhaps, or am I just mis\",\n",
              "  1: 'The forces and accelerations involved in doing a little bit of orbital\\nmaneuvering with HST aboard a'},\n",
              " {0: \"I'm afraid I was not able to find the GIFs... is the list \\nupdated weekly, perhaps, or am I just mis\",\n",
              "  1: 'The forces and accelerations involved in doing a little bit of orbital\\nmaneuvering with HST aboard a'},\n",
              " {0: \"I'm afraid I was not able to find the GIFs... is the list \\nupdated weekly, perhaps, or am I just mis\",\n",
              "  1: 'The forces and accelerations involved in doing a little bit of orbital\\nmaneuvering with HST aboard a'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Ym1h18hXIT",
        "outputId": "c168456c-b25b-4fd0-f7f4-d7e2f913b5ff"
      },
      "source": [
        "# Examples of parsed words by different tokenizers\r\n",
        "[LLsWT[i][0:9] for i in range(len(LLsWT))]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " [\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " [\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " [\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " ['I', 'm', 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'m\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'m\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'m\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'\", 'm', 'afraid', 'I', 'was', 'not', 'able', 'to'],\n",
              " ['I', \"'\", 'm', 'afraid', 'I', 'was', 'not', 'able', 'to']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jat8b-wDtcUz"
      },
      "source": [
        "## **P1. Sentence Tokenizers**\n",
        "\n",
        "Compute the mean (average) length of a sentence (as a count of characters) for each tokenizer and save it to `LnMeanSentLen`. Compute the difference between maximal mean length and minimal mean length.\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> \n",
        "Notice the drastic difference in sentence lengths among these tokenizers. Wow! Investigate the shortest and longest sentences. Are they parsed correctly? What sentence separator can be used to tokenize these correctly? Which sentence tokenizer appears most/least reliable. Which one is fastest (if you tried timing these)? Are there tuning parameters for poorly performing parsers to improve their tokenization results? How would you pre-process the corpus to improve sentence tokenization?\n",
        "<br><i>Hints</i>: The answer is in [100, 200] interval. It might be easier if you use list comprehensions and <code>np.mean()</code> function</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTa_m2y7_ohU"
      },
      "source": [
        "# Example. LnMeanSentLen=[6.0, 14.0]. Then max - min of these values yields 8. \r\n",
        "LLsST_ = [['5char','7  char'],['5char','7  char','a sentence with 30 characters ']]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLc9ragHUQ4J"
      },
      "source": [
        "num_st_tokenizers = len(LLsST)\n",
        "LnMeanSetD = {}\n",
        "for tkn_num in range(num_st_tokenizers):\n",
        "  num_st = len(LLsST[tkn_num])\n",
        "  LnMeanSetD[tkn_num] = []\n",
        "  for st_idx in range(num_st):\n",
        "    LnMeanSetD[tkn_num].append(len(LLsST[tkn_num][st_idx]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF4Y4fJw8zuF",
        "outputId": "bb92f292-921f-482a-dd87-4d4f88a70d5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " LnMeanSetLen = {}\n",
        " max_, min_ = (float(\"-inf\"), float(\"inf\"))\n",
        " for i in range(len(LnMeanSetD)):\n",
        "   LnMeanSetLen[i] = np.mean(LnMeanSetD[i])\n",
        "   max_, min_ = max(max_, LnMeanSetLen[i]), min(min_,  LnMeanSetLen[i])\n",
        "LnMeanSetLen, max_, min_, max_ - min_"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({0: 212.84369449378332,\n",
              "  1: 136.66975666280416,\n",
              "  2: 137.19651162790697,\n",
              "  3: 79.57915567282322},\n",
              " 212.84369449378332,\n",
              " 79.57915567282322,\n",
              " 133.2645388209601)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EkkAL5muFBB"
      },
      "source": [
        "## **P2. Word Tokenizers**\n",
        "\n",
        "Consider а list of lists of word strings, <code>LLsWT</code>, created above. <b>Compute</b> <code>abs(nMaxWordLength-nMaxWordCount)</code>\n",
        "\n",
        "1. `nMaxWordCount` = max count of tokens among each tokenizer.\n",
        "1. `nMaxWordLength` = max character length among all words from all tokenizers.\n",
        "\n",
        "--------------\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Are the longest and shortest word tokens meaningful? If not, should we avoid these or pre-process these in some way?\n",
        "<br><i>Hint:</i> The answer is in the [23000, 24000] interval. It is easier to process <code>LLsWT</code> via loops, list comprehensions or Pandas dataframes.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1t-vWSw_VNJ"
      },
      "source": [
        "# Example: nMaxWordCount=9 and nMaxWordLength=6. The absolute difference is |9-6| or 3\r\n",
        "LLsWT_ =  [['I', \"'m\", 'afraid'], ['I', \"'\", 'm', 'afraid', 'I', 'was', 'not', 'able', 'to']]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkHGM5TaUQWS"
      },
      "source": [
        "num_st_tokenizers = len(LLsWT)\n",
        "LnWordLenSet = {}\n",
        "LnNumWordSet = {}\n",
        "for tkn_num in range(num_st_tokenizers):\n",
        "  num_st = len(LLsWT[tkn_num])\n",
        "  LnWordLenSet[tkn_num] = []\n",
        "  LnNumWordSet[tkn_num] = len(LLsWT[tkn_num])\n",
        "  for wd_idx in range(num_st):\n",
        "    LnWordLenSet[tkn_num].append(len(LLsWT[tkn_num][wd_idx]))\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFbzJEXOJD6b"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxJ6XR4hKFe4",
        "outputId": "5429d00a-ac38-4fcc-c939-db0e7fdf0194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MaxWordLenSet = {i: max(LnWordLenSet[i]) for i in range(len(LnWordLenSet.keys()))}\n",
        "print(MaxWordLenSet)\n",
        "nMaxWordLength = max(MaxWordLenSet.values())\n",
        "print(nMaxWordLength)\n",
        "nMaxWordCount = max(LnNumWordSet.values())\n",
        "print(nMaxWordCount)\n",
        "print('diff', abs(nMaxWordCount - nMaxWordLength))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 77, 1: 77, 2: 77, 3: 77, 4: 18, 5: 36, 6: 36, 7: 155, 8: 77, 9: 77}\n",
            "155\n",
            "23816\n",
            "diff 23661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXFPT_PevOwr"
      },
      "source": [
        "## **P3. Word Tokenizers - Largest Word**\n",
        "\n",
        "Consider a list of lists <code>LLsWT</code> created above. <b>Compute</b> the length of the longest token containing <a href=http://sticksandstones.kstrom.com/appen.html>ASCII</a> letters, i.e. any of a-z or A-Z.\n",
        "\n",
        "<p><i>Takeaway:</i> What additional preprocessing would you include to avoid semantic-less word tokens in your resulting vocabulary?\n",
        "\n",
        "-----------\n",
        "\n",
        "<font color=gray><p><i>Hint:</i> The answer is in the [0, 200] interval. You may need to flatten a list of lists of strings into just a list of strings for convenience. Then remove words that do not contain any ASCII letters. This can be done with the `search()` method from `re` object and `[a-zA-Z]` pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia3Bs-GO_Jud"
      },
      "source": [
        "# Example: The longest word token containing `[a-zA-Z]` is `'here.'` and has length 5.\r\n",
        "LLsWT_ = [['I', \"'\", 'm', 'here', '. (2021)'], ['I\\'m', 'here.', '(2021)']]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXWva9QYUPxJ",
        "outputId": "e4866e5c-dc33-487d-e243-03629965c676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "comp = re.compile(r'[a-zA-Z]')\n",
        "max_len, max_word = 0, ''\n",
        "for tkn_list in LLsWT:\n",
        "  for word in tkn_list:\n",
        "    # print('Word', word)\n",
        "    match = comp.match(word)\n",
        "    if match:\n",
        "      if len(word) > max_len: max_word = word\n",
        "      max_len = max(max_len, len(word))\n",
        "      \n",
        "\n",
        "print('Largest Word', max_word, max_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Largest Word nic.funet.fi:/pub/astro/general/astroftp.txt 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-17dHoXtUgv"
      },
      "source": [
        "## **P4. Dimension Reduction: Lower Casing**\n",
        "\n",
        "Here we compare the effectiveness of lower casing of a text corpus. Consider news text from `LoadNews(['rec.motorcycles'])` with original and lower casing. Compute the **percentage decrease in vocabulary size** of word tokens with and without lowercase pre-processing. Use `nltk.WordPunctTokenizer()` object to parse into word tokens. Then do the same for *each newsgroup element* of <code>LsTgtNames</code> list. Submit the median of these quantities.\n",
        "\n",
        "If original and processed vocab sizes are <code>a</code> and <code>b</code>, then we want <code>(a-b)/a*100</code> as the percent decrease in vocab size.\n",
        "\n",
        "**Toy example 1:** A string <code>\"r you R user?\"</code> has a vocabulary (i.e. unique word tokens) of 4 words; and its lower-case counterpart <code>\"r you r user?\"</code> has a vocabulary of 3 words (because \"R\" was replaced with \"r\"). So, the percentage decrease in vocabulary is (4-3)/4*100=25%. Submit 25\n",
        "\n",
        "**Example 2:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "|group|#orig_words|#lowcase_words|%improvement|\n",
        "|-----|-----------|--------------|--------|\n",
        "|rec.motorcycles|2712|2484|8.41|\n",
        "|comp.sys.mac.hardware|2384|2183|8.43|\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> Notice a healthy reduction in vocabulary size across these newsgroups by lower casing the text. Also, a median is a better measure of centrality (or average) than mean because the former is robust to extreme values (or outliers). FYI: <code>nltk.WordPunctTokenizer()</code> is fast, but offers poorer quality than Spacy's <code>nlp</code> model.\n",
        "\n",
        "-------\n",
        "\n",
        "<font color=gray><i>Hint:</i> First write your code for a single news group; then wrap it into a function; then call this function for every newsgroup. It's simpler to apply lower casing before tokenization. Note vocabulary is always a container of unique words.\n",
        "\n",
        "<font color=gray>Just like a real vocabulary or dictionary, a digital vocabulary also contains unique words only. A big challenge in NLP is handling (storing and processing) huge vocabularies. In classical NLP we normalize words to reduce the vocabulary size with a minimal \"loss of information\". So, if your vocabulary contains 1 million words, then each word is essentially a 1 million dimensional one-hot vector (of zeros and a single 1 identifying the unique position of the word). If we reduce the vocabulary to 100K words, then each word is a 100K dimensional one-hot vector. Naturally, with shorter vector representations we can fit more words in compute memory and do more NLP magic. Pre-processing helps us reduce the vocabulary size.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s28oqjTNPDmo"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNejdEa8Porl"
      },
      "source": [
        "df_improvement = pd.DataFrame(columns=['group', 'orig_words',\t'#lowercase_words',\t'%improvement'])\n",
        "pd.set_option('display.max_rows', 85)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7muZyn_Qkhl"
      },
      "source": [
        "def compute_improvement(news_group):\n",
        "  news = LoadNews([news_group])\n",
        "  token_list = nltk.WordPunctTokenizer().tokenize(news)\n",
        "  org_word_freq = Counter(token_list)\n",
        "  num_org_word = len(org_word_freq)\n",
        "  news_lower = news.lower()\n",
        "  token_list_lower = nltk.WordPunctTokenizer().tokenize(news_lower)\n",
        "  lower_word_freq = Counter(token_list_lower)\n",
        "  num_lower_word = len(lower_word_freq)\n",
        "  return {'group': news_group, \n",
        "          'orig_words': num_org_word, \n",
        "          '#lowercase_words': num_lower_word,\n",
        "          '%improvement': ((num_org_word - num_lower_word)/num_org_word) * 100}\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjXjssMTTcFJ"
      },
      "source": [
        "result = []\n",
        "for news_group in LsTgtNames:\n",
        "  df_improvement = df_improvement.append(compute_improvement(news_group), ignore_index=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VACbEKxhWy0-",
        "outputId": "14afaf09-d4c8-4a68-90b3-90af0a2f1088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "df_improvement.sort_values(by='%improvement')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>orig_words</th>\n",
              "      <th>#lowercase_words</th>\n",
              "      <th>%improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>2712</td>\n",
              "      <td>2484</td>\n",
              "      <td>8.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "      <td>2384</td>\n",
              "      <td>2183</td>\n",
              "      <td>8.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>talk.religion.misc</td>\n",
              "      <td>4450</td>\n",
              "      <td>4068</td>\n",
              "      <td>8.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rec.autos</td>\n",
              "      <td>2935</td>\n",
              "      <td>2679</td>\n",
              "      <td>8.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>rec.sport.hockey</td>\n",
              "      <td>4026</td>\n",
              "      <td>3669</td>\n",
              "      <td>8.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>talk.politics.mideast</td>\n",
              "      <td>5704</td>\n",
              "      <td>5188</td>\n",
              "      <td>9.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rec.sport.baseball</td>\n",
              "      <td>3499</td>\n",
              "      <td>3174</td>\n",
              "      <td>9.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>soc.religion.christian</td>\n",
              "      <td>5101</td>\n",
              "      <td>4614</td>\n",
              "      <td>9.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>talk.politics.misc</td>\n",
              "      <td>5255</td>\n",
              "      <td>4751</td>\n",
              "      <td>9.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>4928</td>\n",
              "      <td>4444</td>\n",
              "      <td>9.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sci.med</td>\n",
              "      <td>5095</td>\n",
              "      <td>4591</td>\n",
              "      <td>9.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "      <td>2860</td>\n",
              "      <td>2564</td>\n",
              "      <td>10.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>sci.electronics</td>\n",
              "      <td>3274</td>\n",
              "      <td>2925</td>\n",
              "      <td>10.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>talk.politics.guns</td>\n",
              "      <td>3895</td>\n",
              "      <td>3464</td>\n",
              "      <td>11.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>sci.crypt</td>\n",
              "      <td>3772</td>\n",
              "      <td>3346</td>\n",
              "      <td>11.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sci.space</td>\n",
              "      <td>4929</td>\n",
              "      <td>4354</td>\n",
              "      <td>11.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>comp.windows.x</td>\n",
              "      <td>4163</td>\n",
              "      <td>3659</td>\n",
              "      <td>12.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "      <td>3330</td>\n",
              "      <td>2894</td>\n",
              "      <td>13.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>misc.forsale</td>\n",
              "      <td>3827</td>\n",
              "      <td>3320</td>\n",
              "      <td>13.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comp.graphics</td>\n",
              "      <td>4971</td>\n",
              "      <td>4311</td>\n",
              "      <td>13.28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       group orig_words #lowercase_words  %improvement\n",
              "8   rec.motorcycles           2712       2484             8.41        \n",
              "4   comp.sys.mac.hardware     2384       2183             8.43        \n",
              "19  talk.religion.misc        4450       4068             8.58        \n",
              "7   rec.autos                 2935       2679             8.72        \n",
              "10  rec.sport.hockey          4026       3669             8.87        \n",
              "17  talk.politics.mideast     5704       5188             9.05        \n",
              "9   rec.sport.baseball        3499       3174             9.29        \n",
              "15  soc.religion.christian    5101       4614             9.55        \n",
              "18  talk.politics.misc        5255       4751             9.59        \n",
              "0   alt.atheism               4928       4444             9.82        \n",
              "13  sci.med                   5095       4591             9.89        \n",
              "3   comp.sys.ibm.pc.hardware  2860       2564             10.35       \n",
              "12  sci.electronics           3274       2925             10.66       \n",
              "16  talk.politics.guns        3895       3464             11.07       \n",
              "11  sci.crypt                 3772       3346             11.29       \n",
              "14  sci.space                 4929       4354             11.67       \n",
              "5   comp.windows.x            4163       3659             12.11       \n",
              "2   comp.os.ms-windows.misc   3330       2894             13.09       \n",
              "6   misc.forsale              3827       3320             13.25       \n",
              "1   comp.graphics             4971       4311             13.28       "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGuaCLdeYL8F",
        "outputId": "df7d0b98-692c-4142-c88e-3a1e88bad0a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.median(df_improvement['%improvement'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.856739800925276"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4wx0LO8UPHR",
        "outputId": "171ecf32-a7c0-4a9c-92f3-039190c2cb6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "moto_news = LoadNews(['comp.sys.mac.hardware'])\n",
        "print(moto_news[:150])\n",
        "token_list = nltk.WordPunctTokenizer().tokenize(moto_news)\n",
        "org_word_freq = Counter(token_list)\n",
        "print(\"# original Words\", len(org_word_freq))\n",
        "print('Most Common',org_word_freq.most_common()[:100])\n",
        "print(f'Freq of \"After\"', org_word_freq.get('After'))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Don't forget the LAMG (Los Angeles Macintosh Group) BBS! It's the BBS for\n",
            "the largest Mac-only user group in the country now that BMUG is\n",
            "multi-pl\n",
            "# original Words 2384\n",
            "Most Common [('.', 443), ('the', 400), (',', 288), ('to', 223), ('a', 206), ('I', 205), (\"'\", 151), ('-', 150), ('and', 143), ('of', 127), ('it', 114), ('that', 105), ('is', 103), ('you', 88), ('(', 86), ('with', 85), ('on', 78), ('?', 76), ('in', 68), ('have', 68), ('for', 66), ('can', 62), ('s', 53), ('t', 52), ('be', 50), ('The', 48), (')', 47), ('\"', 47), ('not', 47), ('are', 45), ('but', 41), ('this', 41), ('or', 37), (':', 37), ('as', 35), ('Apple', 32), ('/', 32), ('was', 31), ('at', 31), ('an', 30), ('one', 29), ('$', 28), ('Mac', 26), ('my', 26), ('from', 26), ('any', 26), ('when', 26), ('if', 26), ('has', 26), ('there', 25), ('do', 25), ('monitor', 24), ('get', 24), ('about', 24), ('all', 24), ('your', 23), ('will', 23), ('which', 23), ('!', 22), ('up', 22), ('just', 22), ('out', 22), ('It', 21), (').', 21), ('problem', 21), ('If', 21), ('so', 21), ('me', 21), ('know', 20), ('>', 20), ('like', 20), ('1', 20), ('2', 20), ('they', 19), ('need', 19), ('computer', 19), ('some', 19), ('would', 19), ('use', 18), ('what', 18), ('IIsi', 18), ('video', 18), ('we', 17), ('This', 17), ('by', 16), ('time', 16), ('other', 16), ('anyone', 16), ('want', 16), ('then', 16), ('|', 16), ('more', 15), ('does', 15), ('SCSI', 15), ('had', 15), ('...', 15), ('been', 14), ('@', 14), ('don', 14), ('*', 14)]\n",
            "Freq of \"After\" None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1-Q8jRLN-O",
        "outputId": "6ce3c081-3071-48f7-9f1b-b8131ef12e85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "moto_news_lower = moto_news.lower()\n",
        "print(moto_news_lower[:150])\n",
        "token_list_lower = nltk.WordPunctTokenizer().tokenize(moto_news_lower)\n",
        "lower_word_freq = Counter(token_list_lower)\n",
        "print(\"# Lowercase Words\", len(lower_word_freq))\n",
        "print('Most Common',lower_word_freq.most_common()[:100])\n",
        "print(f'Freq of \"after\"', lower_word_freq.get('after'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "don't forget the lamg (los angeles macintosh group) bbs! it's the bbs for\n",
            "the largest mac-only user group in the country now that bmug is\n",
            "multi-pl\n",
            "# Lowercase Words 2183\n",
            "Most Common [('the', 449), ('.', 443), (',', 288), ('to', 224), ('a', 215), ('i', 208), (\"'\", 151), ('-', 150), ('and', 144), ('it', 137), ('of', 127), ('is', 110), ('that', 106), ('you', 97), ('(', 86), ('with', 85), ('on', 78), ('?', 76), ('in', 73), ('can', 71), ('have', 69), ('for', 68), ('s', 59), ('this', 58), ('t', 53), ('not', 51), ('be', 50), ('are', 49), (')', 47), ('\"', 47), ('if', 47), ('but', 42), ('or', 42), ('mac', 38), ('any', 38), ('as', 38), (':', 37), ('apple', 36), ('at', 35), ('was', 32), ('/', 32), ('my', 31), ('so', 31), ('do', 31), ('has', 31), ('one', 30), ('an', 30), ('there', 28), ('monitor', 28), ('when', 28), ('$', 28), ('from', 27), ('all', 27), ('what', 26), ('they', 24), ('get', 24), ('about', 24), ('will', 24), ('we', 24), ('your', 23), ('which', 23), ('!', 22), ('does', 22), ('up', 22), ('just', 22), ('out', 22), ('computer', 22), ('would', 22), (').', 21), ('problem', 21), ('me', 21), ('anyone', 20), ('know', 20), ('>', 20), ('like', 20), ('1', 20), ('2', 20), ('some', 20), ('need', 19), ('scsi', 18), ('use', 18), ('iisi', 18), ('video', 18), ('off', 17), ('drive', 17), ('then', 17), ('don', 16), ('by', 16), ('time', 16), ('other', 16), ('want', 16), ('had', 16), ('|', 16), ('more', 15), ('...', 15), ('he', 15), ('been', 14), ('@', 14), ('am', 14), ('*', 14)]\n",
            "Freq of \"after\" 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFopfbOKvcdC"
      },
      "source": [
        "## **P5. Dimension Reduction: Contraction Expansion (CE)**\n",
        "\n",
        "Similar to P4, measure the percent decrease in vocabulary size across multiple news groups due to the application of CE with the <code>contractions</code> package (without modifying). Apply CE before tokenization. See w3 Colab notebook.\n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "\n",
        "|group|#orig_words|#CE_words|%improvement|\n",
        "|--|--|--|--|\n",
        "|talk.politics.mideast|5704|5685|0.33|\n",
        "|soc.religion.christian|5101|\t5083|\t0.35|\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> As expected, the CE is not as effective, but is complementary to lower casing and other techniques. Typically,we create pre-processing pipelines, where the order matters. For example, given \"I'm here\", CE+parsing yields <code>['I','am','here']</code>, while parsing+CE yields <code>['I am', 'here']</code>.\n",
        "\n",
        "---------------------\n",
        "\n",
        "<font color=\"gray\">Notice that we are measuring the percent decrease in vocabulary size due to application of contraction expansion only. The previous preprocessing of lower casing is not used here, since it would make it more difficult to measure the effect of expansion along.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLUguZMtUSXr"
      },
      "source": [
        "cMap = contractions.contractions_dict\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOLz_SdICBwn",
        "outputId": "8b09ffac-dad8-4134-a16f-1571aa9c0729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "moto_news = LoadNews(['soc.religion.christian'])\n",
        "#print(moto_news[:150])\n",
        "token_list = nltk.WordPunctTokenizer().tokenize(moto_news)\n",
        "org_word_freq = Counter(token_list)\n",
        "print(\"# original Words\", len(org_word_freq))\n",
        "#print('Most Common',org_word_freq.most_common()[:100])\n",
        "\n",
        "\n",
        "moto_news_exp = contractions.fix(moto_news)\n",
        "#print(moto_news_exp[:150])\n",
        "token_list_exp = nltk.WordPunctTokenizer().tokenize(moto_news_exp)\n",
        "exp_word_freq = Counter(token_list_exp)\n",
        "print(\"# original Words\", len(exp_word_freq))\n",
        "#print('Most Common',exp_word_freq.most_common()[:100])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# original Words 5101\n",
            "# original Words 5083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bJYXOMyDjm4"
      },
      "source": [
        "df_improvement_exp = pd.DataFrame(columns=['group', 'orig_words',\t'#CE_words',\t'%improvement'])\n",
        "pd.set_option('display.max_rows', 85)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjaml4KHDwy8"
      },
      "source": [
        "def compute_exp_improvement(news_group):\n",
        "  news = LoadNews([news_group])\n",
        "  token_list = nltk.WordPunctTokenizer().tokenize(news)\n",
        "  org_word_freq = Counter(token_list)\n",
        "  num_org_word = len(org_word_freq)\n",
        "  \n",
        "  news_exp = contractions.fix(news)\n",
        "  token_list_exp = nltk.WordPunctTokenizer().tokenize(news_exp)\n",
        "  exp_word_freq = Counter(token_list_exp)\n",
        "  num_exp_word = len(exp_word_freq)\n",
        "  return {'group': news_group, \n",
        "          'orig_words': num_org_word, \n",
        "          '#CE_words': num_exp_word,\n",
        "          '%improvement': ((num_org_word - num_exp_word)/num_org_word) * 100}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MahqKOxyDrlL"
      },
      "source": [
        "result = []\n",
        "for news_group in LsTgtNames:\n",
        "  df_improvement_exp = df_improvement_exp.append(compute_exp_improvement(news_group), ignore_index=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVCPAW3kEhLn",
        "outputId": "e8ca58db-e00b-4f67-f2eb-a504a9f93105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "df_improvement_exp.sort_values(by='%improvement')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>orig_words</th>\n",
              "      <th>#CE_words</th>\n",
              "      <th>%improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>talk.politics.mideast</td>\n",
              "      <td>5704</td>\n",
              "      <td>5685</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>soc.religion.christian</td>\n",
              "      <td>5101</td>\n",
              "      <td>5083</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sci.space</td>\n",
              "      <td>4929</td>\n",
              "      <td>4910</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>talk.politics.misc</td>\n",
              "      <td>5255</td>\n",
              "      <td>5232</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>misc.forsale</td>\n",
              "      <td>3827</td>\n",
              "      <td>3809</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>talk.religion.misc</td>\n",
              "      <td>4450</td>\n",
              "      <td>4428</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>4928</td>\n",
              "      <td>4901</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>comp.windows.x</td>\n",
              "      <td>4163</td>\n",
              "      <td>4139</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comp.graphics</td>\n",
              "      <td>4971</td>\n",
              "      <td>4942</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sci.med</td>\n",
              "      <td>5095</td>\n",
              "      <td>5065</td>\n",
              "      <td>0.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "      <td>2860</td>\n",
              "      <td>2841</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>talk.politics.guns</td>\n",
              "      <td>3895</td>\n",
              "      <td>3869</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rec.sport.baseball</td>\n",
              "      <td>3499</td>\n",
              "      <td>3474</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>rec.sport.hockey</td>\n",
              "      <td>4026</td>\n",
              "      <td>3997</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>sci.electronics</td>\n",
              "      <td>3274</td>\n",
              "      <td>3250</td>\n",
              "      <td>0.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "      <td>3330</td>\n",
              "      <td>3305</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>sci.crypt</td>\n",
              "      <td>3772</td>\n",
              "      <td>3742</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>2712</td>\n",
              "      <td>2687</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "      <td>2384</td>\n",
              "      <td>2359</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rec.autos</td>\n",
              "      <td>2935</td>\n",
              "      <td>2904</td>\n",
              "      <td>1.06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       group orig_words #CE_words  %improvement\n",
              "17  talk.politics.mideast     5704       5685      0.33        \n",
              "15  soc.religion.christian    5101       5083      0.35        \n",
              "14  sci.space                 4929       4910      0.39        \n",
              "18  talk.politics.misc        5255       5232      0.44        \n",
              "6   misc.forsale              3827       3809      0.47        \n",
              "19  talk.religion.misc        4450       4428      0.49        \n",
              "0   alt.atheism               4928       4901      0.55        \n",
              "5   comp.windows.x            4163       4139      0.58        \n",
              "1   comp.graphics             4971       4942      0.58        \n",
              "13  sci.med                   5095       5065      0.59        \n",
              "3   comp.sys.ibm.pc.hardware  2860       2841      0.66        \n",
              "16  talk.politics.guns        3895       3869      0.67        \n",
              "9   rec.sport.baseball        3499       3474      0.71        \n",
              "10  rec.sport.hockey          4026       3997      0.72        \n",
              "12  sci.electronics           3274       3250      0.73        \n",
              "2   comp.os.ms-windows.misc   3330       3305      0.75        \n",
              "11  sci.crypt                 3772       3742      0.80        \n",
              "8   rec.motorcycles           2712       2687      0.92        \n",
              "4   comp.sys.mac.hardware     2384       2359      1.05        \n",
              "7   rec.autos                 2935       2904      1.06        "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUqlFzNzHJFR",
        "outputId": "54d9af3d-85ec-41ad-a567-0b1605027a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.median(df_improvement_exp['%improvement'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6265741128351531"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNC29OfvlT5"
      },
      "source": [
        "## **P6. Dimension Reduction: Stopwords Removal**\n",
        "\n",
        "Similar to P4, measure the percent decrease in vocabulary size across multiple news groups due to the removal of stop words. Use <code>LsStopWords</code> defined above. Remove stopwords after tokenization. See w3 Colab.\n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "\n",
        "|group|\t#orig_words|\t#important_words|\t%improvement|\n",
        "|--|--|--|--|\n",
        "|talk.politics.mideast|\t5704|\t5568|\t2.38|\n",
        "|comp.graphics|\t4971|\t4842|\t2.60|\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Are you surprised by the average percent of stop words in these corpora?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Z1rre0CbpK"
      },
      "source": [
        "def tokenize_news(sNews):\n",
        "  LLWTs = [sNews.split()] \\\n",
        "      + [nltk.RegexpTokenizer(pattern=r\"\\s+\", gaps=True ).tokenize(sNews)] \\\n",
        "      + [nltk.RegexpTokenizer(pattern=r\"\\s+\", gaps=True ).tokenize(sNews)] \\\n",
        "      + [nltk.WhitespaceTokenizer().tokenize(sNews)] \\\n",
        "      + [nltk.RegexpTokenizer(pattern=r\"\\w+\", gaps=False).tokenize(sNews)] \\\n",
        "      + [nltk.word_tokenize(sNews)] \\\n",
        "      + [nltk.TreebankWordTokenizer().tokenize(sNews)] \\\n",
        "      + [[t.text for t in nlp(sNews)]] \\\n",
        "      + [nltk.tokenize.toktok.ToktokTokenizer().tokenize(sNews)] \\\n",
        "      + [nltk.WordPunctTokenizer().tokenize(sNews)]\n",
        "\n",
        "  return LLWTs"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFI4RulqUTUs"
      },
      "source": [
        "def remove_stop_words(tokens):\n",
        "  return [str(w) for w in tokens if not str(w) in set(LsStopWords)]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouB6EivSPHdH"
      },
      "source": [
        "moto_news = LoadNews(['comp.graphics'])\n",
        "# moto_news[:100]\n",
        "# type(moto_news)\n",
        "# token_list = nltk.WordPunctTokenizer().tokenize(moto_news)\n",
        "token_lists = tokenize_news(moto_news)\n",
        "#token_list = nltk.tokenize.PunktSentenceTokenizer().tokenize(moto_news)\n",
        "#token_list\n",
        "token_lists_wo_stop_words = []\n",
        "for token_list in token_lists:\n",
        "  news_wo_stop_words = remove_stop_words(token_list)\n",
        "  token_lists_wo_stop_words.append(news_wo_stop_words)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIMR3Ca-KtJM",
        "outputId": "eff9db45-3717-46e8-9f88-1e0afece3986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in  range(len(token_lists_wo_stop_words)):\n",
        "  org_word_freq = Counter(token_lists[i])\n",
        "  word_freq_wo_stop_words = Counter(token_lists_wo_stop_words[i])\n",
        "  print(\"# of original Words:{}, # important_words: {} %improvement: {}\".format(len(org_word_freq), len(word_freq_wo_stop_words),((len(org_word_freq) - len(word_freq_wo_stop_words)) / len(org_word_freq)) * 100))\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of original Words:6139, # important_words: 6013 %improvement: 2.0524515393386547\n",
            "# of original Words:6139, # important_words: 6013 %improvement: 2.0524515393386547\n",
            "# of original Words:6139, # important_words: 6013 %improvement: 2.0524515393386547\n",
            "# of original Words:6139, # important_words: 6013 %improvement: 2.0524515393386547\n",
            "# of original Words:4824, # important_words: 4695 %improvement: 2.674129353233831\n",
            "# of original Words:4978, # important_words: 4858 %improvement: 2.4106066693451185\n",
            "# of original Words:5350, # important_words: 5231 %improvement: 2.2242990654205608\n",
            "# of original Words:5005, # important_words: 4889 %improvement: 2.317682317682318\n",
            "# of original Words:5338, # important_words: 5210 %improvement: 2.397901835893593\n",
            "# of original Words:4971, # important_words: 4842 %improvement: 2.595051297525649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMFtzcLlfoSO"
      },
      "source": [
        "df_improvement_stop = pd.DataFrame(columns=['group', 'orig_words',\t'#important_words',\t'%improvement'])\n",
        "pd.set_option('display.max_rows', 85)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpkTvUaLkdw7"
      },
      "source": [
        "def compute_improvement_wo_stop_words(news_group):\n",
        "  news = LoadNews([news_group])\n",
        "  token_list = nltk.WordPunctTokenizer().tokenize(news)\n",
        "  org_word_freq = Counter(token_list)\n",
        "  num_org_word = len(org_word_freq)\n",
        "  \n",
        "  token_list_wo_stop = remove_stop_words(token_list)\n",
        "  token_list_wo_stop_freq = Counter(token_list_wo_stop)\n",
        "  num_word_wo_stop = len(token_list_wo_stop_freq)\n",
        "  return {'group': news_group, \n",
        "          'orig_words': num_org_word, \n",
        "          '#important_words': num_word_wo_stop,\n",
        "          '%improvement': ((num_org_word - num_word_wo_stop)/num_org_word) * 100}"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDs3wuQQmcvq"
      },
      "source": [
        "result = []\n",
        "for news_group in LsTgtNames:\n",
        "  df_improvement_stop = df_improvement_stop.append(compute_improvement_wo_stop_words(news_group), ignore_index=True)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aplIm719mlNV",
        "outputId": "12b30e4b-3648-412f-8fad-15861cf5fd2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "df_improvement_stop.sort_values(by='%improvement')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>orig_words</th>\n",
              "      <th>#important_words</th>\n",
              "      <th>%improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>talk.politics.mideast</td>\n",
              "      <td>5704</td>\n",
              "      <td>5568</td>\n",
              "      <td>2.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comp.graphics</td>\n",
              "      <td>4971</td>\n",
              "      <td>4842</td>\n",
              "      <td>2.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sci.med</td>\n",
              "      <td>5095</td>\n",
              "      <td>4960</td>\n",
              "      <td>2.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>talk.politics.misc</td>\n",
              "      <td>5255</td>\n",
              "      <td>5114</td>\n",
              "      <td>2.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sci.space</td>\n",
              "      <td>4929</td>\n",
              "      <td>4795</td>\n",
              "      <td>2.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>soc.religion.christian</td>\n",
              "      <td>5101</td>\n",
              "      <td>4961</td>\n",
              "      <td>2.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>4928</td>\n",
              "      <td>4786</td>\n",
              "      <td>2.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>misc.forsale</td>\n",
              "      <td>3827</td>\n",
              "      <td>3711</td>\n",
              "      <td>3.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>comp.windows.x</td>\n",
              "      <td>4163</td>\n",
              "      <td>4034</td>\n",
              "      <td>3.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>talk.religion.misc</td>\n",
              "      <td>4450</td>\n",
              "      <td>4309</td>\n",
              "      <td>3.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>rec.sport.hockey</td>\n",
              "      <td>4026</td>\n",
              "      <td>3894</td>\n",
              "      <td>3.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>sci.crypt</td>\n",
              "      <td>3772</td>\n",
              "      <td>3640</td>\n",
              "      <td>3.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>talk.politics.guns</td>\n",
              "      <td>3895</td>\n",
              "      <td>3758</td>\n",
              "      <td>3.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rec.sport.baseball</td>\n",
              "      <td>3499</td>\n",
              "      <td>3366</td>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>sci.electronics</td>\n",
              "      <td>3274</td>\n",
              "      <td>3145</td>\n",
              "      <td>3.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "      <td>3330</td>\n",
              "      <td>3198</td>\n",
              "      <td>3.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rec.autos</td>\n",
              "      <td>2935</td>\n",
              "      <td>2805</td>\n",
              "      <td>4.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "      <td>2860</td>\n",
              "      <td>2730</td>\n",
              "      <td>4.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>2712</td>\n",
              "      <td>2578</td>\n",
              "      <td>4.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "      <td>2384</td>\n",
              "      <td>2261</td>\n",
              "      <td>5.16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       group orig_words #important_words  %improvement\n",
              "17  talk.politics.mideast     5704       5568             2.38        \n",
              "1   comp.graphics             4971       4842             2.60        \n",
              "13  sci.med                   5095       4960             2.65        \n",
              "18  talk.politics.misc        5255       5114             2.68        \n",
              "14  sci.space                 4929       4795             2.72        \n",
              "15  soc.religion.christian    5101       4961             2.74        \n",
              "0   alt.atheism               4928       4786             2.88        \n",
              "6   misc.forsale              3827       3711             3.03        \n",
              "5   comp.windows.x            4163       4034             3.10        \n",
              "19  talk.religion.misc        4450       4309             3.17        \n",
              "10  rec.sport.hockey          4026       3894             3.28        \n",
              "11  sci.crypt                 3772       3640             3.50        \n",
              "16  talk.politics.guns        3895       3758             3.52        \n",
              "9   rec.sport.baseball        3499       3366             3.80        \n",
              "12  sci.electronics           3274       3145             3.94        \n",
              "2   comp.os.ms-windows.misc   3330       3198             3.96        \n",
              "7   rec.autos                 2935       2805             4.43        \n",
              "3   comp.sys.ibm.pc.hardware  2860       2730             4.55        \n",
              "8   rec.motorcycles           2712       2578             4.94        \n",
              "4   comp.sys.mac.hardware     2384       2261             5.16        "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38v7-1uLmolT",
        "outputId": "e08dfb8a-cb7a-4deb-9a6e-7d0aafd27aec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.median(df_improvement_stop['%improvement'])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2236139252164304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpFUq0J4vr6T"
      },
      "source": [
        "## **P7. Dimension Reduction: Normalizing Accented Characters (NAC)**\n",
        "\n",
        "Similar to P4, measure the percent decrease in vocabulary size across multiple news groups due to the removal of accent marks as we did in Module 3. Use <code>LsStopWords</code> defined above to retrieve smaller vocabularies. First do NAC, then tokenize. See w3 Colab. \n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "\n",
        "|group|\t#orig_words|\t#NAC_tokens|\t%improvement|\n",
        "|--|--|--|--|\n",
        "|alt.atheism|\t4928|\t4928|\t0.0|\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Does the answer surprise you? Why do you think you are seeing this percentage reduction?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIGjRqdhkdCY",
        "outputId": "7cc9f07e-bc6f-4c89-d94d-2b9d27c02a41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "moto_news = LoadNews(['alt.atheism'])\n",
        "normalized_text = unicodedata.normalize('NFKD', moto_news).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "token_list = nltk.WordPunctTokenizer().tokenize(normalized_text)\n",
        "len(Counter(token_list))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgX91oWPUUmk"
      },
      "source": [
        "df_improvement_nac = pd.DataFrame(columns=['group', 'orig_words',\t'#NAC_tokens',\t'%improvement'])\n",
        "pd.set_option('display.max_rows', 85)\n",
        "\n",
        "def compute_improvement_nac(news_group):\n",
        "  news = LoadNews([news_group])\n",
        "  token_list = nltk.WordPunctTokenizer().tokenize(news)\n",
        "  org_word_freq = Counter(token_list)\n",
        "  num_org_word = len(org_word_freq)\n",
        "  \n",
        "  normalized_text = unicodedata.normalize('NFKD', news).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  token_list_nac = nltk.WordPunctTokenizer().tokenize(normalized_text)\n",
        "  token_list_nac_freq = Counter(token_list_nac)\n",
        "  num_word_nac = len(token_list_nac_freq)\n",
        "  return {'group': news_group, \n",
        "          'orig_words': num_org_word, \n",
        "          '#NAC_tokens': num_word_nac,\n",
        "          '%improvement': ((num_org_word - num_word_nac)/num_org_word) * 100}"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVOJPhyUtXRk",
        "outputId": "05e5830b-4e61-4488-8449-6b53a9b0d0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "result = []\n",
        "for news_group in LsTgtNames:\n",
        "  df_improvement_nac = df_improvement_nac.append(compute_improvement_nac(news_group), ignore_index=True)\n",
        "\n",
        "df_improvement_nac.sort_values(by='%improvement')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>orig_words</th>\n",
              "      <th>#NAC_tokens</th>\n",
              "      <th>%improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>4928</td>\n",
              "      <td>4928</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>talk.politics.mideast</td>\n",
              "      <td>5704</td>\n",
              "      <td>5704</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>talk.politics.guns</td>\n",
              "      <td>3895</td>\n",
              "      <td>3895</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>soc.religion.christian</td>\n",
              "      <td>5101</td>\n",
              "      <td>5101</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sci.space</td>\n",
              "      <td>4929</td>\n",
              "      <td>4929</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sci.med</td>\n",
              "      <td>5095</td>\n",
              "      <td>5095</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>sci.electronics</td>\n",
              "      <td>3274</td>\n",
              "      <td>3274</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>sci.crypt</td>\n",
              "      <td>3772</td>\n",
              "      <td>3772</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>rec.sport.hockey</td>\n",
              "      <td>4026</td>\n",
              "      <td>4026</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rec.sport.baseball</td>\n",
              "      <td>3499</td>\n",
              "      <td>3499</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>2712</td>\n",
              "      <td>2712</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rec.autos</td>\n",
              "      <td>2935</td>\n",
              "      <td>2935</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>misc.forsale</td>\n",
              "      <td>3827</td>\n",
              "      <td>3827</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>comp.windows.x</td>\n",
              "      <td>4163</td>\n",
              "      <td>4163</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "      <td>2384</td>\n",
              "      <td>2384</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "      <td>2860</td>\n",
              "      <td>2860</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "      <td>3330</td>\n",
              "      <td>3330</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comp.graphics</td>\n",
              "      <td>4971</td>\n",
              "      <td>4971</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>talk.politics.misc</td>\n",
              "      <td>5255</td>\n",
              "      <td>5255</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>talk.religion.misc</td>\n",
              "      <td>4450</td>\n",
              "      <td>4450</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       group orig_words #NAC_tokens  %improvement\n",
              "0   alt.atheism               4928       4928        0.0         \n",
              "17  talk.politics.mideast     5704       5704        0.0         \n",
              "16  talk.politics.guns        3895       3895        0.0         \n",
              "15  soc.religion.christian    5101       5101        0.0         \n",
              "14  sci.space                 4929       4929        0.0         \n",
              "13  sci.med                   5095       5095        0.0         \n",
              "12  sci.electronics           3274       3274        0.0         \n",
              "11  sci.crypt                 3772       3772        0.0         \n",
              "10  rec.sport.hockey          4026       4026        0.0         \n",
              "9   rec.sport.baseball        3499       3499        0.0         \n",
              "8   rec.motorcycles           2712       2712        0.0         \n",
              "7   rec.autos                 2935       2935        0.0         \n",
              "6   misc.forsale              3827       3827        0.0         \n",
              "5   comp.windows.x            4163       4163        0.0         \n",
              "4   comp.sys.mac.hardware     2384       2384        0.0         \n",
              "3   comp.sys.ibm.pc.hardware  2860       2860        0.0         \n",
              "2   comp.os.ms-windows.misc   3330       3330        0.0         \n",
              "1   comp.graphics             4971       4971        0.0         \n",
              "18  talk.politics.misc        5255       5255        0.0         \n",
              "19  talk.religion.misc        4450       4450        0.0         "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CmrL0bwtk34",
        "outputId": "ac4b4971-a9bc-4106-aa39-b8a068388876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.median(df_improvement_nac['%improvement'])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7Fr4xAvzi5"
      },
      "source": [
        "## **P8. Dimension Reduction: Porter Stemmer vs WordNet Lemmatizer**\n",
        "\n",
        "Similar to P4, we compare the effectiveness of stemmers and lemmatizers in reducing the vocabulary size. \n",
        "\n",
        "Apply the stemmer <code>pso</code> (defined above) to the tokenized words of each news group (as we did in Module 3 video and Jupyter Notebook). Then compute the median percent decrease in vocabulary size. Then apply the lemmatizer <code>wlo</code> (defined above) the same way. Then submit the largest of the two percentage decrease values.\n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "|group|#orig_words|#pso_words|%improvement|\n",
        "|-|-|-|-|\n",
        "|rec.sport.hockey|4026|3271|18.75|\n",
        "|rec.motorcycles|2712|2158|20.43|\n",
        "\n",
        "|group|#orig_words|#wso_words|%improvement|\n",
        "|-|-|-|-|\n",
        "|misc.forsale|3827|3708|3.11|\n",
        "|rec.sport.hockey|4026|3897|3.20|\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Notice the more aggressive normalizer. Do you think it produces higher quality words (that can still be found in a common English dictionary)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Jmu--UUWA9"
      },
      "source": [
        "df_improvement_stemmer = pd.DataFrame(columns=['group', '#orig_words',\t'#stem_lem_words',\t'%improvement', 'stemmer'])\n",
        "pd.set_option('display.max_rows', 85)\n",
        "\n",
        "def compute_improvement_w_stemming(news_group, stem_lem, stemmer_name):\n",
        "  news = LoadNews([news_group])\n",
        "  token_list = nltk.WordPunctTokenizer().tokenize(news)\n",
        "  org_word_freq = Counter(token_list)\n",
        "  num_org_word = len(org_word_freq)\n",
        "  \n",
        "  if stemmer_name == 'pso':\n",
        "    token_list_stemmed = [stem_lem.stem(w) for w in token_list]\n",
        "  else:\n",
        "    token_list_stemmed = [stem_lem.lemmatize(w) for w in token_list]\n",
        "  token_list_stemmed_freq = Counter(token_list_stemmed)\n",
        "  num_word_wo_stop = len(token_list_stemmed_freq)\n",
        "  return {'stemmer': stemmer_name,\n",
        "          'group': news_group, \n",
        "          '#orig_words': num_org_word, \n",
        "          '#stem_lem_words': num_word_wo_stop,\n",
        "          '%improvement': ((num_org_word - num_word_wo_stop)/num_org_word) * 100}"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvA683Ip2A8W"
      },
      "source": [
        "result = []\n",
        "stemmer_dict = {'pso': pso, 'wlo': wlo}\n",
        "for name, stemmer in stemmer_dict.items():\n",
        "  for news_group in LsTgtNames:\n",
        "    df_improvement_stemmer = df_improvement_stemmer.append(compute_improvement_w_stemming(news_group, stemmer, name), ignore_index=True)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsoz6IIz94Gl"
      },
      "source": [
        "df_improvement_stemmer_grp = df_improvement_stemmer.groupby(['stemmer'])\n",
        "#df_improvement_stemmer.sort_values(by=['%improvement'], ascending=False)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8odbjcyS8w5",
        "outputId": "c0b96f3f-bf1d-480a-f4bd-1e4d45de727e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#rec.sport.hockey\t4026\t3271\t18.75\n",
        "#rec.sport.hockey\t4026\t3897\t3.20\n",
        "df_pso = df_improvement_stemmer_grp.get_group('pso')\n",
        "np.median(df_pso['%improvement'])\n",
        "#df_pso"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.65413661583169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkgxrD20U1CC",
        "outputId": "3ab488dc-7d26-47d0-a0d7-59e1199c47ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_wlo = df_improvement_stemmer_grp.get_group('wlo')\n",
        "np.median(df_wlo['%improvement'])\n",
        "#df_wlo"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.425927181355558"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1x9mGxjTUxc",
        "outputId": "0da954a2-0053-445e-fce4-e6525bbe3077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# rec.motorcycles\t2712\t2158\t20.43\n",
        "df_improvement_stemmer_grp.get_group('rec.motorcycles')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>#orig_words</th>\n",
              "      <th>#stem_lem_words</th>\n",
              "      <th>%improvement</th>\n",
              "      <th>stemmer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>2712</td>\n",
              "      <td>2158</td>\n",
              "      <td>20.43</td>\n",
              "      <td>pso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>2712</td>\n",
              "      <td>2583</td>\n",
              "      <td>4.76</td>\n",
              "      <td>wlo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              group #orig_words #stem_lem_words  %improvement stemmer\n",
              "8   rec.motorcycles  2712        2158            20.43         pso   \n",
              "28  rec.motorcycles  2712        2583            4.76          wlo   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFR-msK8Tyui",
        "outputId": "dddcfb55-635e-4847-9126-394b74c5aee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# misc.forsale 3827\t3708\t3.11\n",
        "df_improvement_stemmer_grp.get_group('misc.forsale')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>#orig_words</th>\n",
              "      <th>#stem_lem_words</th>\n",
              "      <th>%improvement</th>\n",
              "      <th>stemmer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>misc.forsale</td>\n",
              "      <td>3827</td>\n",
              "      <td>3014</td>\n",
              "      <td>21.24</td>\n",
              "      <td>pso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>misc.forsale</td>\n",
              "      <td>3827</td>\n",
              "      <td>3708</td>\n",
              "      <td>3.11</td>\n",
              "      <td>wlo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           group #orig_words #stem_lem_words  %improvement stemmer\n",
              "6   misc.forsale  3827        3014            21.24         pso   \n",
              "26  misc.forsale  3827        3708            3.11          wlo   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQreNqwlCe_P",
        "outputId": "2434526d-3f95-4629-adf8-e8d3a8b908bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "for news_group in LsTgtNames:\n",
        "  df_improvement_stemmer.get_group(news_group)\n",
        "\n",
        "df_improvement_stemmer.sort_values(by)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9ca9394d470d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnews_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLsTgtNames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdf_improvement_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_improvement_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'get_group'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CelnySRwv5S8"
      },
      "source": [
        "## **P9. Dimension Reduction: Measure Quality of Stems/Lemmas**\n",
        "\n",
        "Here we are quantitatively evaluating the quality of contributions to vocabulary from **stemming** and **lemmatization**. Many of the stemmed and lemmatized words may not be proper English words. We assess whether stems and lemmas are spelled correctly by checking whether they appear in the set of 56K word vocabulary from **Brown corpus**, which is <code>SsBrownVcb</code> set object created above.\n",
        "\n",
        "<p>Given one newsgroup corpus, say <code>'misc.forsale'</code>, compute <code>vocab_orig</code> set of tokens from the original text and <code>vocab_pso</code> set of stemmed tokens using <code>pso</code> object. Then compute a set of newly formed word tokens, <code>new_tokens_pso</code>, that were not in the original set. Now, how many of these new stems are in the Brown vocabulary? Compute the percent of new stems which are also found in Brown corpus (vs all new stems from <code>pso</code>). Let's call this quality metric <code>accuracy_pso</code>.\n",
        "\n",
        "<p>Now, compute <code>accuracy_wlo</code> similarly, but with a lemmatizer object <code>wlo</code> created above. \n",
        "\n",
        "<p>Compute the absolute difference, <code>abs(accuracy_wlo-accuracy_pso)</code> and call it <code>abs_acc_diff</code>.\n",
        "\n",
        "<p>Finally, compute the median of all <code>abs_acc_diff</code> metrics across all newsgroups in <code>LsTgtNames</code> list.\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> Notice the drastic difference in quality between two techniques.\n",
        "\n",
        "---------------\n",
        "\n",
        "<i>Hint:</i> You will need to use set operations here. Sets are extremely effective for testing memberships of members of one group in another group. See Corey Schafer video on sets, if you need a refresher. </font>\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Toy example 1:** Say you have a sentence \"NLP is awesomely exciting\", which tokenizes to <font color=blue>[\"NLP\", \"is\", \"awesomely\", \"exciting\"]</font>, which stem to (for example) <font color=blue>[\"NLP\", \"is\", \"awesome\", \"excit\"]</font>. Now, there are 2 new words and, suppose, only one of them is correct. That is <font color=blue>\"awesome\"</font> is a new word that is correct and <font color=blue>\"excit\"</font> is incorrect. \n",
        "\n",
        "<p>So, we only have 50% of the new words, which are correct. Then we would expect lemmatization to yield 100% of new words to be correct. How do we determine whether the word is correct or not? We could spell-check, but it's slow. We could also check if it's in some dictionary of words that we consider correct, such as Oxford Dictionary, Wikipedia, etc. Well, often we just use Brown vocabulary as that ground-truth set of words. So, in this exercise we need to compute the percent of new words found in Brown corpus (vs all new stems from pso).\n",
        "\n",
        "<p>Further clarification:\n",
        "\n",
        "1. Let S:= set of new words resulting from stemming (i.e. new stems)\n",
        "1. Let B:= set of all words in Brown vocabulary (these are unique words)\n",
        "1. Let BS:= the intersection of the B and S. That is all new words found in Brown corpus\n",
        "1. len(BS) is the size of this intersection, i.e. the count of elements in the BS set.\n",
        "1. We need to find len(BS)/len(S) as a percentage.\n",
        "\n",
        "<p>Finally, we compute the absolute difference between quality metrics, scale it up to all newsgroups, and then pick the median for submission.\n",
        "\n",
        "You should observe these intermediate results. Here the rows ordered by `abs_acc_diff`:\n",
        "\n",
        "|newsgroups|acc_diff_pso|acc_diff_wlo|abs_acc_diff|\n",
        "|--|--|--|--|\n",
        "|misc.forsale|30.07|80.49|50.42|\n",
        "|rec.motorcycles|26.52|79.00|52.48|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt2Ac7OhUXD2"
      },
      "source": [
        "df_stemmer_compare = pd.DataFrame(columns=['newsgroups', 'acc_diff_pso',\t'acc_diff_wlo',\t'abs_acc_diff'])\n",
        "pd.set_option('display.max_rows', 85)\n",
        "from collections import defaultdict"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdKXE8AlLc5C"
      },
      "source": [
        "def compute_improvement_w_stemming(news_group):\n",
        "  news = LoadNews([news_group])\n",
        "  token_list = nltk.WordPunctTokenizer().tokenize(news.lower())\n",
        "  org_word_freq = Counter(token_list)\n",
        "  num_org_word = len(org_word_freq)\n",
        "  set_of_orig_words = set(token_list)\n",
        "  #print('Set of original words', set_of_orig_words)\n",
        "\n",
        "  stemmer_dict = {'pso': pso, 'wlo': wlo}\n",
        "  token_list_stemmed = defaultdict(set)\n",
        "  for name, stemmer in stemmer_dict.items():\n",
        "    if name == 'pso':\n",
        "      token_list_from_stemming = [stemmer.stem(w) for w in token_list]\n",
        "      #print('stemmed pso', token_list_from_stemming)\n",
        "    else:\n",
        "      token_list_from_stemming = [stemmer.lemmatize(w) for w in token_list]\n",
        "      #print('stemmed wlo', token_list_from_stemming)\n",
        "    \n",
        "    stemmed_word_freq = Counter(token_list_from_stemming)\n",
        "    token_list_stemmed[name] = set(token_list_from_stemming)\n",
        "\n",
        "  \n",
        "  new_words_per_stemmer = defaultdict(set)\n",
        "  for name in stemmer_dict.keys():\n",
        "    new_words_per_stemmer[name] = token_list_stemmed[name] - set_of_orig_words\n",
        "\n",
        "  return new_words_per_stemmer"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnoiYVc7MyMn"
      },
      "source": [
        "#d = compute_improvement_w_stemming(\"\")\n",
        "results = {}\n",
        "for news_group in ['misc.forsale', 'rec.motorcycles']: #LsTgtNames:\n",
        "  results[news_group] = compute_improvement_w_stemming(news_group)\n",
        "# token_list_stemmed_freq = Counter(token_list_stemmed)\n",
        "# num_word_wo_stop = len(token_list_stemmed_freq)\n",
        "# return {'stemmer': stemmer_name,\n",
        "#         'group': news_group, \n",
        "#         '#orig_words': num_org_word, \n",
        "#         '#stem_lem_words': num_word_wo_stop,\n",
        "#         '%improvement': ((num_org_word - num_word_wo_stop)/num_org_word) * 100}"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54qcWmLLVIkc"
      },
      "source": [
        "bs1 = results['misc.forsale']['pso'].intersection(SsBrownVcb)\n",
        "bs2 = results['rec.motorcycles']['pso'].intersection(SsBrownVcb)\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efbeW7BTdoxf",
        "outputId": "81637cdf-49dd-42f7-97bb-7d864261a1c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(len(bs1)/len(results['misc.forsale']['pso'])) * 100"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.122762148337596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHqAlM8-ekKc"
      },
      "source": [
        "for news_group, result in results.items():\n",
        "  bs_pso = result['pso'].intersection(SsBrownVcb)\n",
        "  bs_wlo = result['wlo'].intersection(SsBrownVcb)\n",
        "  acc_diff_pso = (len(bs_pso)/len(result['pso'])) * 100\n",
        "  acc_diff_wlo = (len(bs_wlo)/len(result['wlo'])) * 100\n",
        "  #print('acc_diff_pso: {}\tacc_diff_wlo:\t{} abs_acc_diff: {}'.format(acc_diff_pso, acc_diff_wlo, abs(acc_diff_wlo - acc_diff_pso)))\n",
        "  result_news_group = {\n",
        "      'newsgroups': news_group, \n",
        "      'acc_diff_pso': acc_diff_pso, \n",
        "      'acc_diff_wlo': acc_diff_wlo,\n",
        "      'abs_acc_diff': abs(acc_diff_wlo - acc_diff_pso)\n",
        "      }\n",
        "  df_stemmer_compare = df_stemmer_compare.append(result_news_group, ignore_index=True)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOdtftTG8TwM",
        "outputId": "b2cb4cb2-8651-4d30-b955-1139ba60b0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "df_stemmer_compare"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>newsgroups</th>\n",
              "      <th>acc_diff_pso</th>\n",
              "      <th>acc_diff_wlo</th>\n",
              "      <th>abs_acc_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>20.18</td>\n",
              "      <td>82.02</td>\n",
              "      <td>61.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comp.graphics</td>\n",
              "      <td>16.90</td>\n",
              "      <td>75.00</td>\n",
              "      <td>58.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "      <td>20.34</td>\n",
              "      <td>81.32</td>\n",
              "      <td>60.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "      <td>22.66</td>\n",
              "      <td>84.88</td>\n",
              "      <td>62.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "      <td>23.29</td>\n",
              "      <td>86.59</td>\n",
              "      <td>63.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>comp.windows.x</td>\n",
              "      <td>17.09</td>\n",
              "      <td>79.81</td>\n",
              "      <td>62.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>misc.forsale</td>\n",
              "      <td>22.12</td>\n",
              "      <td>78.26</td>\n",
              "      <td>56.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>rec.autos</td>\n",
              "      <td>21.82</td>\n",
              "      <td>84.31</td>\n",
              "      <td>62.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>rec.motorcycles</td>\n",
              "      <td>24.52</td>\n",
              "      <td>78.64</td>\n",
              "      <td>54.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rec.sport.baseball</td>\n",
              "      <td>23.84</td>\n",
              "      <td>73.64</td>\n",
              "      <td>49.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>rec.sport.hockey</td>\n",
              "      <td>23.04</td>\n",
              "      <td>80.00</td>\n",
              "      <td>56.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>sci.crypt</td>\n",
              "      <td>20.06</td>\n",
              "      <td>84.56</td>\n",
              "      <td>64.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>sci.electronics</td>\n",
              "      <td>20.57</td>\n",
              "      <td>80.65</td>\n",
              "      <td>60.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>sci.med</td>\n",
              "      <td>17.63</td>\n",
              "      <td>76.92</td>\n",
              "      <td>59.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sci.space</td>\n",
              "      <td>19.74</td>\n",
              "      <td>80.81</td>\n",
              "      <td>61.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>soc.religion.christian</td>\n",
              "      <td>16.92</td>\n",
              "      <td>72.82</td>\n",
              "      <td>55.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>talk.politics.guns</td>\n",
              "      <td>22.34</td>\n",
              "      <td>80.11</td>\n",
              "      <td>57.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>talk.politics.mideast</td>\n",
              "      <td>21.95</td>\n",
              "      <td>77.51</td>\n",
              "      <td>55.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>talk.politics.misc</td>\n",
              "      <td>20.48</td>\n",
              "      <td>81.44</td>\n",
              "      <td>60.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>talk.religion.misc</td>\n",
              "      <td>20.88</td>\n",
              "      <td>79.70</td>\n",
              "      <td>58.81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  newsgroups  acc_diff_pso  acc_diff_wlo  abs_acc_diff\n",
              "0   alt.atheism               20.18         82.02         61.84       \n",
              "1   comp.graphics             16.90         75.00         58.10       \n",
              "2   comp.os.ms-windows.misc   20.34         81.32         60.98       \n",
              "3   comp.sys.ibm.pc.hardware  22.66         84.88         62.22       \n",
              "4   comp.sys.mac.hardware     23.29         86.59         63.29       \n",
              "5   comp.windows.x            17.09         79.81         62.71       \n",
              "6   misc.forsale              22.12         78.26         56.14       \n",
              "7   rec.autos                 21.82         84.31         62.50       \n",
              "8   rec.motorcycles           24.52         78.64         54.12       \n",
              "9   rec.sport.baseball        23.84         73.64         49.80       \n",
              "10  rec.sport.hockey          23.04         80.00         56.96       \n",
              "11  sci.crypt                 20.06         84.56         64.50       \n",
              "12  sci.electronics           20.57         80.65         60.08       \n",
              "13  sci.med                   17.63         76.92         59.29       \n",
              "14  sci.space                 19.74         80.81         61.07       \n",
              "15  soc.religion.christian    16.92         72.82         55.90       \n",
              "16  talk.politics.guns        22.34         80.11         57.77       \n",
              "17  talk.politics.mideast     21.95         77.51         55.56       \n",
              "18  talk.politics.misc        20.48         81.44         60.96       \n",
              "19  talk.religion.misc        20.88         79.70         58.81       "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adnNMQdI6q1U",
        "outputId": "2fe5ce7c-6100-40c4-db26-a8bc09e1894a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.median(df_stemmer_compare['abs_acc_diff'])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59.68398086237613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9IY1DaTliPa"
      },
      "source": [
        "# acc_diff_wlo = (len(bs_wlo)/len(result['wlo'])\n",
        "#df_stemmer_compare = df_stemmer_compare.append(compute_improvement_w_stemming(news_group, stemmer, name), ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlJj7ErpmdzF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}