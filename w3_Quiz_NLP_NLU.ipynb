{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w3_Quiz_NLP_NLU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaykrishna2013/NLP/blob/main/w3_Quiz_NLP_NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdR2I2GktWFP"
      },
      "source": [
        "<font color=brown><i><b>Copyright Information:</b></i> All course material is copyrighted by the author and Stanford University with all rights reserved. The material may not be reproduced or distributed without an explicit written permission of the author. Stanford University also has a policy that one should know before audio/video recording. Do not post material online.</font>\n",
        "\n",
        "# **w3 Quiz. NLP and NLU**\n",
        "\n",
        "Make a copy of this Colab Notebook with the **starter code** below and continue building your solution **in Colab** (not another Python environment) to assure an exact environment and matching solutions.\n",
        "\n",
        "Prof. Melnikov's video in module 3 demonstrates several ways to **tokenize sentences** and **words**. Here we evaluate their effectiveness in reducing the dimensionality of the vocabulary, while maintaining the \"quality\" of the tokens. We define \"quality\" with a binary function with *high* quality for words found in some commonly accepted vocabulary (such as [**Brown corpus**](https://www.nltk.org/book/ch02.html) from **NLTK library**). Other words are considered as *low* quality. This will require Python **set operations**, since we need to check whether a given word is in the vocabulary set or not.\n",
        "\n",
        "Most evaluations here are done one the first 100 posts from each of the [**20 Newsgroups corpus**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) in **Scikit-Learn library**. We then use the average metric (**median**, in this case) to compute the final quantity, which you will submit to Canvas Quiz.\n",
        "\n",
        "This quiz is a sequence of small projects, requiring computation of values. Do not round any values. If stuck for 10-15 minutes, raise a question in the w3 Discussion Q&A. See Syllabus for **Posting Guidelines**. In particular, describe your situation and what you have tried **without posting your code**.\n",
        "\n",
        "To ease code readability  and avoid confusion, we use the following prefix convention for key variables: \n",
        "1. `s`=string, `n`=number, `b`=Boolean\n",
        "1. `Ls`=list of strings, `Ln`=list of numbers, `Ss`=set of strings, `As`=NumPy array of strings, `Ds`=dictionary of string values\n",
        "1. `LLs`=list of lists of strings, `LTs`=list of tuples of strings, etc.\n",
        "1. `df`=Pandas dataframe or series, \n",
        "\n",
        "<p><font color=gray><i>Hint</i>: Refer to videos in Module 3 and some Python refresher videos of Corey Schafer.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8k-vxn4q62h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5dff51-0657-4fed-f374-90621db95814"
      },
      "source": [
        "!pip -q install contractions   # quietly install contractions package\n",
        "# allows multiple outputs from a single Colab code cell:\n",
        "from IPython.core.interactiveshell import InteractiveShell  \n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import sys, matplotlib.pylab as plt, re, platform, matplotlib\n",
        "import numpy as np, pandas as pd, nltk, sklearn, spacy, unicodedata, contractions \n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from pprint import pprint\n",
        "tmp = nltk.download(['brown', 'stopwords','punkt','wordnet'], quiet=True) # See https://www.nltk.org/book/ch02.html\n",
        "LsStopWords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Increase viewable area of Pandas tables, NumPy arrays, plots\n",
        "pd.set_option('max_rows', 5, 'max_columns', 500, 'max_colwidth', 1, 'precision', 2)\n",
        "np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True)\n",
        "plt.rcParams['figure.figsize'] = [16, 4]\n",
        "\n",
        "def LoadNews(cat=['sci.space'], TopN=100):\n",
        "    '''Function to load a string of news posts for the specified categories. Returns: TopN concatenated news'''\n",
        "    Rem = ('headers', 'footers', 'quotes')  # remove these fields from result set\n",
        "    bunch = fetch_20newsgroups(categories=cat, subset='test', shuffle=False, remove=Rem)\n",
        "    return '\\n'.join(bunch.data[:TopN])  # save first 100 posts concatenated as a single string.\n",
        "\n",
        "# See doc: https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset\n",
        "# We preload string variables containing concatenated news posts \n",
        "sNews = LoadNews(['sci.space'])   # a string. news from space ;)  \n",
        "LsTgtNames = list(fetch_20newsgroups().target_names)   # names of 20 newsgroups\n",
        "\n",
        "pso = nltk.stem.PorterStemmer()       # instantiates Porter Stemmer object\n",
        "wlo = nltk.stem.WordNetLemmatizer()   # instantiates WordNet lemmatizer object\n",
        "SsBrownVcb = set(brown.words())       # Vocabulary of 56057 words in Brown Corpus\n",
        "\n",
        "# store sentence tokenizers' results as a list of lists or strings:\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "LLsST =  [sNews.split('. ')] \\\n",
        "    + [nltk.sent_tokenize(sNews)] \\\n",
        "    + [nltk.tokenize.PunktSentenceTokenizer().tokenize(sNews)] \\\n",
        "    + [[n.text for n in nlp(sNews).sents]]\n",
        "\n",
        "# store word tokenizers' results as a list of lists of strings\n",
        "LLsWT = [sNews.split()] \\\n",
        "    + [nltk.RegexpTokenizer(pattern=r\"\\s+\", gaps=True ).tokenize(sNews)] \\\n",
        "    + [nltk.RegexpTokenizer(pattern=r\"\\s+\", gaps=True ).tokenize(sNews)] \\\n",
        "    + [nltk.WhitespaceTokenizer().tokenize(sNews)] \\\n",
        "    + [nltk.RegexpTokenizer(pattern=r\"\\w+\", gaps=False).tokenize(sNews)] \\\n",
        "    + [nltk.word_tokenize(sNews)] \\\n",
        "    + [nltk.TreebankWordTokenizer().tokenize(sNews)] \\\n",
        "    + [[t.text for t in nlp(sNews)]] \\\n",
        "    + [nltk.tokenize.toktok.ToktokTokenizer().tokenize(sNews)] \\\n",
        "    + [nltk.WordPunctTokenizer().tokenize(sNews)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 266kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 327kB 11.7MB/s \n",
            "\u001b[?25h  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAFHkUTPgViv",
        "outputId": "1d2df34c-ac6e-4e3f-ae1f-d10943efcf13"
      },
      "source": [
        "# Example of news article\r\n",
        "pprint(sNews[0:100])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\"I'm afraid I was not able to find the GIFs... is the list \\n\"\n",
            " 'updated weekly, perhaps, or am I just mis')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDFXaBILgjFP",
        "outputId": "ae74310b-c72f-43da-de03-e0c950777d44"
      },
      "source": [
        "# Examples of parsed sentences by different tokenizers\r\n",
        "[{i:LLsST[j][i][:100] for i in [0,1]} for j in range(len(LLsST))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{0: \"I'm afraid I was not able to find the GIFs..\",\n",
              "  1: 'is the list \\nupdated weekly, perhaps, or am I just missing something?\\n\\nThe forces and accelerations '},\n",
              " {0: \"I'm afraid I was not able to find the GIFs... is the list \\nupdated weekly, perhaps, or am I just mis\",\n",
              "  1: 'The forces and accelerations involved in doing a little bit of orbital\\nmaneuvering with HST aboard a'},\n",
              " {0: \"I'm afraid I was not able to find the GIFs... is the list \\nupdated weekly, perhaps, or am I just mis\",\n",
              "  1: 'The forces and accelerations involved in doing a little bit of orbital\\nmaneuvering with HST aboard a'},\n",
              " {0: I'm afraid I was not able to find the GIFs... is the list \n",
              "  updated weekly, perhaps, or am I just missing something?\n",
              "  , 1: The forces and accelerations involved in doing a little bit of orbital\n",
              "  maneuvering with HST aboard are much smaller than those involved in\n",
              "  reentry, landing, and re-launch.  }]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Ym1h18hXIT",
        "outputId": "4eb03022-22ee-43c6-ef44-ca6748b107aa"
      },
      "source": [
        "# Examples of parsed words by different tokenizers\r\n",
        "[LLsWT[i][0:9] for i in range(len(LLsWT))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " [\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " [\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " [\"I'm\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find', 'the'],\n",
              " ['I', 'm', 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'m\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'m\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'m\", 'afraid', 'I', 'was', 'not', 'able', 'to', 'find'],\n",
              " ['I', \"'\", 'm', 'afraid', 'I', 'was', 'not', 'able', 'to'],\n",
              " ['I', \"'\", 'm', 'afraid', 'I', 'was', 'not', 'able', 'to']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jat8b-wDtcUz"
      },
      "source": [
        "## **P1. Sentence Tokenizers**\n",
        "\n",
        "Compute the mean (average) length of a sentence (as a count of characters) for each tokenizer and save it to `LnMeanSentLen`. Compute the difference between maximal mean length and minimal mean length.\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> \n",
        "Notice the drastic difference in sentence lengths among these tokenizers. Wow! Investigate the shortest and longest sentences. Are they parsed correctly? What sentence separator can be used to tokenize these correctly? Which sentence tokenizer appears most/least reliable. Which one is fastest (if you tried timing these)? Are there tuning parameters for poorly performing parsers to improve their tokenization results? How would you pre-process the corpus to improve sentence tokenization?\n",
        "<br><i>Hints</i>: The answer is in [100, 200] interval. It might be easier if you use list comprehensions and <code>np.mean()</code> function</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTa_m2y7_ohU"
      },
      "source": [
        "# Example. LnMeanSentLen=[6.0, 14.0]. Then max - min of these values yields 8. \r\n",
        "LLsST_ = [['5char','7  char'],['5char','7  char','a sentence with 30 characters ']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLc9ragHUQ4J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EkkAL5muFBB"
      },
      "source": [
        "## **P2. Word Tokenizers**\n",
        "\n",
        "Consider а list of lists of word strings, <code>LLsWT</code>, created above. <b>Compute</b> <code>abs(nMaxWordLength-nMaxWordCount)</code>\n",
        "\n",
        "1. `nMaxWordCount` = max count of tokens among each tokenizer.\n",
        "1. `nMaxWordLength` = max character length among all words from all tokenizers.\n",
        "\n",
        "--------------\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Are the longest and shortest word tokens meaningful? If not, should we avoid these or pre-process these in some way?\n",
        "<br><i>Hint:</i> The answer is in the [23000, 24000] interval. It is easier to process <code>LLsWT</code> via loops, list comprehensions or Pandas dataframes.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1t-vWSw_VNJ"
      },
      "source": [
        "# Example: nMaxWordCount=9 and nMaxWordLength=6. The absolute difference is |9-6| or 3\r\n",
        "LLsWT_ =  [['I', \"'m\", 'afraid'], ['I', \"'\", 'm', 'afraid', 'I', 'was', 'not', 'able', 'to']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkHGM5TaUQWS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXFPT_PevOwr"
      },
      "source": [
        "## **P3. Word Tokenizers - Largest Word**\n",
        "\n",
        "Consider a list of lists <code>LLsWT</code> created above. <b>Compute</b> the length of the longest token containing <a href=http://sticksandstones.kstrom.com/appen.html>ASCII</a> letters, i.e. any of a-z or A-Z.\n",
        "\n",
        "<p><i>Takeaway:</i> What additional preprocessing would you include to avoid semantic-less word tokens in your resulting vocabulary?\n",
        "\n",
        "-----------\n",
        "\n",
        "<font color=gray><p><i>Hint:</i> The answer is in the [0, 200] interval. You may need to flatten a list of lists of strings into just a list of strings for convenience. Then remove words that do not contain any ASCII letters. This can be done with the `search()` method from `re` object and `[a-zA-Z]` pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia3Bs-GO_Jud"
      },
      "source": [
        "# Example: The longest word token containing `[a-zA-Z]` is `'here.'` and has length 5.\r\n",
        "LLsWT_ = [['I', \"'\", 'm', 'here', '. (2021)'], ['I\\'m', 'here.', '(2021)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXWva9QYUPxJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-17dHoXtUgv"
      },
      "source": [
        "## **P4. Dimension Reduction: Lower Casing**\n",
        "\n",
        "Here we compare the effectiveness of lower casing of a text corpus. Consider news text from `LoadNews(['rec.motorcycles'])` with original and lower casing. Compute the **percentage decrease in vocabulary size** of word tokens with and without lowercase pre-processing. Use `nltk.WordPunctTokenizer()` object to parse into word tokens. Then do the same for *each newsgroup element* of <code>LsTgtNames</code> list. Submit the median of these quantities.\n",
        "\n",
        "If original and processed vocab sizes are <code>a</code> and <code>b</code>, then we want <code>(a-b)/a*100</code> as the percent decrease in vocab size.\n",
        "\n",
        "**Toy example 1:** A string <code>\"r you R user?\"</code> has a vocabulary (i.e. unique word tokens) of 4 words; and its lower-case counterpart <code>\"r you r user?\"</code> has a vocabulary of 3 words (because \"R\" was replaced with \"r\"). So, the percentage decrease in vocabulary is (4-3)/4*100=25%. Submit 25\n",
        "\n",
        "**Example 2:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "|group|#orig_words|#lowcase_words|%improvement|\n",
        "|-----|-----------|--------------|--------|\n",
        "|rec.motorcycles|2712|2484|8.41|\n",
        "|comp.sys.mac.hardware|2384|2183|8.43|\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> Notice a healthy reduction in vocabulary size across these newsgroups by lower casing the text. Also, a median is a better measure of centrality (or average) than mean because the former is robust to extreme values (or outliers). FYI: <code>nltk.WordPunctTokenizer()</code> is fast, but offers poorer quality than Spacy's <code>nlp</code> model.\n",
        "\n",
        "-------\n",
        "\n",
        "<font color=gray><i>Hint:</i> First write your code for a single news group; then wrap it into a function; then call this function for every newsgroup. It's simpler to apply lower casing before tokenization. Note vocabulary is always a container of unique words.\n",
        "\n",
        "<font color=gray>Just like a real vocabulary or dictionary, a digital vocabulary also contains unique words only. A big challenge in NLP is handling (storing and processing) huge vocabularies. In classical NLP we normalize words to reduce the vocabulary size with a minimal \"loss of information\". So, if your vocabulary contains 1 million words, then each word is essentially a 1 million dimensional one-hot vector (of zeros and a single 1 identifying the unique position of the word). If we reduce the vocabulary to 100K words, then each word is a 100K dimensional one-hot vector. Naturally, with shorter vector representations we can fit more words in compute memory and do more NLP magic. Pre-processing helps us reduce the vocabulary size.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4wx0LO8UPHR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFopfbOKvcdC"
      },
      "source": [
        "## **P5. Dimension Reduction: Contraction Expansion (CE)**\n",
        "\n",
        "Similar to P4, measure the percent decrease in vocabulary size across multiple news groups due to the application of CE with the <code>contractions</code> package (without modifying). Apply CE before tokenization. See w3 Colab notebook.\n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "\n",
        "|group|#orig_words|#CE_words|%improvement|\n",
        "|--|--|--|--|\n",
        "|talk.politics.mideast|5704|5685|0.33|\n",
        "|soc.religion.christian|5101|\t5083|\t0.35|\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> As expected, the CE is not as effective, but is complementary to lower casing and other techniques. Typically,we create pre-processing pipelines, where the order matters. For example, given \"I'm here\", CE+parsing yields <code>['I','am','here']</code>, while parsing+CE yields <code>['I am', 'here']</code>.\n",
        "\n",
        "---------------------\n",
        "\n",
        "<font color=\"gray\">Notice that we are measuring the percent decrease in vocabulary size due to application of contraction expansion only. The previous preprocessing of lower casing is not used here, since it would make it more difficult to measure the effect of expansion along.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLUguZMtUSXr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNC29OfvlT5"
      },
      "source": [
        "## **P6. Dimension Reduction: Stopwords Removal**\n",
        "\n",
        "Similar to P4, measure the percent decrease in vocabulary size across multiple news groups due to the removal of stop words. Use <code>LsStopWords</code> defined above. Remove stopwords after tokenization. See w3 Colab.\n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "\n",
        "|group|\t#orig_words|\t#important_words|\t%improvement|\n",
        "|--|--|--|--|\n",
        "|talk.politics.mideast|\t5704|\t5568|\t2.38|\n",
        "|comp.graphics|\t4971|\t4842|\t2.60|\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Are you surprised by the average percent of stop words in these corpora?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFI4RulqUTUs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpFUq0J4vr6T"
      },
      "source": [
        "## **P7. Dimension Reduction: Normalizing Accented Characters (NAC)**\n",
        "\n",
        "Similar to P4, measure the percent decrease in vocabulary size across multiple news groups due to the removal of accent marks as we did in Module 3. Use <code>LsStopWords</code> defined above to retrieve smaller vocabularies. First do NAC, then tokenize. See w3 Colab. \n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "\n",
        "|group|\t#orig_words|\t#NAC_tokens|\t%improvement|\n",
        "|--|--|--|--|\n",
        "|alt.atheism|\t4928|\t4928|\t0.0|\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Does the answer surprise you? Why do you think you are seeing this percentage reduction?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgX91oWPUUmk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7Fr4xAvzi5"
      },
      "source": [
        "## **P8. Dimension Reduction: Porter Stemmer vs WordNet Lemmatizer**\n",
        "\n",
        "Similar to P4, we compare the effectiveness of stemmers and lemmatizers in reducing the vocabulary size. \n",
        "\n",
        "Apply the stemmer <code>pso</code> (defined above) to the tokenized words of each news group (as we did in Module 3 video and Jupyter Notebook). Then compute the median percent decrease in vocabulary size. Then apply the lemmatizer <code>wlo</code> (defined above) the same way. Then submit the largest of the two percentage decrease values.\n",
        "\n",
        "**Example:** You should observe these intermediate results. Here the rows ordered by `%improvement`:\n",
        "\n",
        "|group|#orig_words|#pso_words|%improvement|\n",
        "|-|-|-|-|\n",
        "|rec.sport.hockey|4026|3271|18.75|\n",
        "|rec.motorcycles|2712|2158|20.43|\n",
        "\n",
        "|group|#orig_words|#wso_words|%improvement|\n",
        "|-|-|-|-|\n",
        "|misc.forsale|3827|3708|3.11|\n",
        "|rec.sport.hockey|4026|3897|3.20|\n",
        "\n",
        "<font color=gray><i>Takeaway:</i> Notice the more aggressive normalizer. Do you think it produces higher quality words (that can still be found in a common English dictionary)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Jmu--UUWA9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CelnySRwv5S8"
      },
      "source": [
        "## **P9. Dimension Reduction: Measure Quality of Stems/Lemmas**\n",
        "\n",
        "Here we are quantitatively evaluating the quality of contributions to vocabulary from **stemming** and **lemmatization**. Many of the stemmed and lemmatized words may not be proper English words. We assess whether stems and lemmas are spelled correctly by checking whether they appear in the set of 56K word vocabulary from **Brown corpus**, which is <code>SsBrownVcb</code> set object created above.\n",
        "\n",
        "<p>Given one newsgroup corpus, say <code>'misc.forsale'</code>, compute <code>vocab_orig</code> set of tokens from the original text and <code>vocab_pso</code> set of stemmed tokens using <code>pso</code> object. Then compute a set of newly formed word tokens, <code>new_tokens_pso</code>, that were not in the original set. Now, how many of these new stems are in the Brown vocabulary? Compute the percent of new stems which are also found in Brown corpus (vs all new stems from <code>pso</code>). Let's call this quality metric <code>accuracy_pso</code>.\n",
        "\n",
        "<p>Now, compute <code>accuracy_wlo</code> similarly, but with a lemmatizer object <code>wlo</code> created above. \n",
        "\n",
        "<p>Compute the absolute difference, <code>abs(accuracy_wlo-accuracy_pso)</code> and call it <code>abs_acc_diff</code>.\n",
        "\n",
        "<p>Finally, compute the median of all <code>abs_acc_diff</code> metrics across all newsgroups in <code>LsTgtNames</code> list.\n",
        "\n",
        "<p><font color=\"gray\"><i>Takeaway:</i> Notice the drastic difference in quality between two techniques.\n",
        "\n",
        "---------------\n",
        "\n",
        "<i>Hint:</i> You will need to use set operations here. Sets are extremely effective for testing memberships of members of one group in another group. See Corey Schafer video on sets, if you need a refresher. </font>\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Toy example 1:** Say you have a sentence \"NLP is awesomely exciting\", which tokenizes to <font color=blue>[\"NLP\", \"is\", \"awesomely\", \"exciting\"]</font>, which stem to (for example) <font color=blue>[\"NLP\", \"is\", \"awesome\", \"excit\"]</font>. Now, there are 2 new words and, suppose, only one of them is correct. That is <font color=blue>\"awesome\"</font> is a new word that is correct and <font color=blue>\"excit\"</font> is incorrect. \n",
        "\n",
        "<p>So, we only have 50% of the new words, which are correct. Then we would expect lemmatization to yield 100% of new words to be correct. How do we determine whether the word is correct or not? We could spell-check, but it's slow. We could also check if it's in some dictionary of words that we consider correct, such as Oxford Dictionary, Wikipedia, etc. Well, often we just use Brown vocabulary as that ground-truth set of words. So, in this exercise we need to compute the percent of new words found in Brown corpus (vs all new stems from pso).\n",
        "\n",
        "<p>Further clarification:\n",
        "\n",
        "1. Let S:= set of new words resulting from stemming (i.e. new stems)\n",
        "1. Let B:= set of all words in Brown vocabulary (these are unique words)\n",
        "1. Let BS:= the intersection of the B and S. That is all new words found in Brown corpus\n",
        "1. len(BS) is the size of this intersection, i.e. the count of elements in the BS set.\n",
        "1. We need to find len(BS)/len(S) as a percentage.\n",
        "\n",
        "<p>Finally, we compute the absolute difference between quality metrics, scale it up to all newsgroups, and then pick the median for submission.\n",
        "\n",
        "You should observe these intermediate results. Here the rows ordered by `abs_acc_diff`:\n",
        "\n",
        "|newsgroups|acc_diff_pso|acc_diff_wlo|abs_acc_diff|\n",
        "|--|--|--|--|\n",
        "|misc.forsale|30.07|80.49|50.42|\n",
        "|rec.motorcycles|26.52|79.00|52.48|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt2Ac7OhUXD2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}